{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of NewAthena Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.patheffects as pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and prepare table for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data file with registrations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regis_file = \"registrations_final_20250416.xlsx\"\n",
    "date_data = \"20251114\"\n",
    "save_figs = True\n",
    "# change dir to the date_data folder\n",
    "os.chdir(f\"/home/ceballos/INSTRUMEN/ATHENA/ACO/Community/{date_data}\")\n",
    "regis_file = f\"registrations_final_{date_data}.xlsx\"\n",
    "# get date from filename\n",
    "date = re.search(r\"_(\\d{8})\", regis_file)\n",
    "if date:\n",
    "    date = date.group(1)\n",
    "print(f\"Using date: {date}\")\n",
    "cleanregis_file = \"cleaned_registrations.xlsx\"\n",
    "# Load data from xlsx file:first row is header\n",
    "df = pd.read_excel(regis_file, header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicates and save to a clean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on a specific column\n",
    "df_cleaned_column = df.drop_duplicates(subset=['Name'], keep='first') \n",
    "\n",
    "# Save the cleaned data to a new Excel file\n",
    "df_cleaned_column.to_excel(cleanregis_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the cleaned_registrations file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from xlsx file:first row is header\n",
    "df = pd.read_excel(cleanregis_file, header=0)\n",
    "# convert NaN to empty string but keep boolean values\n",
    "df = df.where(pd.notnull(df), '')\n",
    "# fill NaN with empty string\n",
    "df = df.fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Title of columns: df_clean0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print names of columns\n",
    "print(df.columns)\n",
    "df_clean0 = df.copy()\n",
    "# change column names:\n",
    "# 'INSTEAD, I'd like to be an ACTIVE MEMBER at:' -> 'ACTIVE_MEMBER'\n",
    "df_clean0.rename(columns={\"INSTEAD, I'd like to be an ACTIVE MEMBER at:\": \"ACTIVE_MEMBER\"}, inplace=True)\n",
    "# 'IN ADDITION, I'd like to be CHAIR at:' -> 'CHAIR_REQUESTED'\n",
    "df_clean0.rename(columns={\"IN ADDITION, I'd like to be CHAIR at:\": \"CHAIR_REQUESTED\"}, inplace=True)\n",
    "# 'I'd prefer being only a SUPPORT MEMBER' -> 'SUPPORT_MEMBER'\n",
    "df_clean0.rename(columns={\"I'd prefer being only a SUPPORT MEMBER\": \"SUPPORT_MEMBER\"}, inplace=True)\n",
    "# email address\n",
    "df_clean0.rename(columns={\"Email Address\": \"EMAIL\"}, inplace=True)\n",
    "print(df_clean0.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify WG labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace strings with WG numbers and WG names as only WG numbers (taking into account that several WGs can be listed in the same cell)\n",
    "# for example: \"WG4: Compact objects\" -> \"WG4\"\n",
    "df_clean = df_clean0.copy()\n",
    "df_clean[\"ACTIVE_MEMBER\"] = df_clean0[\"ACTIVE_MEMBER\"].apply(lambda x: \";\".join(re.findall(r'\\b(WG\\d+)\\b', x)))\n",
    "df_clean[\"CHAIR_REQUESTED\"] = df_clean0[\"CHAIR_REQUESTED\"].apply(lambda x: \";\".join(re.findall(r'\\b(WG\\d+)\\b', x)))\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get subtables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_support = df_clean[df_clean[\"SUPPORT_MEMBER\"] == \"Yes\"]\n",
    "df_chair_req = df_clean[df_clean[\"CHAIR_REQUESTED\"] != \"\"]\n",
    "df_active = df_clean[df_clean[\"ACTIVE_MEMBER\"] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of Members types (Support/Active/Chairs) by country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total number of support members\n",
    "total_support = len(df_support)\n",
    "total_members = len(df_clean)\n",
    "total_chairs_req = len(df_chair_req)\n",
    "total_active = len(df_active)\n",
    "\n",
    "# Get total members per country\n",
    "all_countries = df_clean[\"Country1\"].value_counts()\n",
    "\n",
    "# Sort countries by total members (descending)\n",
    "sorted_countries = all_countries.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n",
    "# Build aligned counts\n",
    "total = df_clean[\"Country1\"].value_counts().reindex(sorted_countries, fill_value=0)\n",
    "active = df_active[\"Country1\"].value_counts().reindex(sorted_countries, fill_value=0)\n",
    "support = df_support[\"Country1\"].value_counts().reindex(sorted_countries, fill_value=0)\n",
    "chair_req = df_chair_req[\"Country1\"].value_counts().reindex(sorted_countries, fill_value=0)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "counts_df = pd.DataFrame({\n",
    "    f'Total members ({total_members})': total,\n",
    "    f'Active members ({total_active})': active,\n",
    "    f'Support members ({total_support})': support,\n",
    "    f'Requested Chair ({total_chairs_req})': chair_req\n",
    "})\n",
    "\n",
    "bars_colors = ['red', 'purple', 'blue', 'green']\n",
    "\n",
    "# Plot and add label to each barplot\n",
    "ax = counts_df.plot(kind='bar', width=0.8, logy=True, figsize=(12, 6), color=bars_colors, alpha=0.5)\n",
    "#ax = counts_df.plot(kind='bar', width=0.8, logy=False, figsize=(12, 6), color=bars_colors, alpha=0.5)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Number of Members\")\n",
    "ax.set_title(\"NASC Membership by Country\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "if save_figs:\n",
    "    plt.savefig(\"NASC_Membership_by_Country.png\", dpi=300, bbox_inches='tight')\n",
    "    #plt.savefig(\"NASC_Membership_by_Country_nolog.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage of community members in each country\n",
    "country_percentage_total = (total / total_members) * 100\n",
    "country_percentage_active = (active / total_active) * 100\n",
    "# sort by percentage\n",
    "country_percentage_total = country_percentage_total.sort_values(ascending=False)\n",
    "country_percentage_active = country_percentage_active.sort_values(ascending=False)\n",
    "# save to csv\n",
    "country_percentage_total.to_csv(\"NASC_Membership_by_Country_percentage.csv\", header=[\"Percentage\"])\n",
    "country_percentage_active.to_csv(\"NASC_Membership_by_Country_active_percentage.csv\", header=[\"Percentage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = \"United Kingdom\"\n",
    "affil = \"\"\n",
    "df_country = df_clean[df_clean[\"Country1\"] == country]\n",
    "if affil:\n",
    "    df_filtered =df_filtered = df_country[df_country[\"Affiliation1\"].str.contains(affil, case=False, na=False)]\n",
    "else:\n",
    "    df_filtered = df_country.copy()\n",
    "#print members from \"country\" in blocks of ten members\n",
    "#for i in range(0, len(df_country), 10):\n",
    "#    print(df_country.iloc[i:i+10][[\"Name\", \"Country1\", \"ACTIVE_MEMBER\", \"SUPPORT_MEMBER\",\"CHAIR_REQUESTED\"]])\n",
    "\n",
    "#print(f\"Members from {country}:\")\n",
    "#print(df_country[[\"Name\", \"Country1\", \"ACTIVE_MEMBER\", \"CHAIR_REQUESTED\"]])\n",
    "print(f\"Total members from {country}: {len(df_country)}\")\n",
    "# print chairs (requested) from country\n",
    "#print(\"Requested Chairs from {country}:\")\n",
    "#print(df_country[df_country[\"CHAIR_REQUESTED\"] != \"\"])\n",
    "print(f\"Total requested chairs from {country}: {len(df_country[df_country['CHAIR_REQUESTED'] != ''])}\")\n",
    "# print active members of country\n",
    "print(f\"Total active members from {country}: {len(df_country[df_country['ACTIVE_MEMBER'] != ''])}\")\n",
    "# print support members of country\n",
    "print(f\"Total support members from {country}: {len(df_country[df_country['SUPPORT_MEMBER'] == 'Yes'])}\")\n",
    "\n",
    "# print member from country who applied for chair but not activer nor support\n",
    "print(f\"Total members from {country} who applied for chair but not active nor support: {len(df_country[(df_country['CHAIR_REQUESTED'] != '') & (df_country['ACTIVE_MEMBER'] == '') & (df_country['SUPPORT_MEMBER'] == 'No')])}\")\n",
    "# print which members from country did not apply for active nor support\n",
    "print(f\"Total members from {country} who did not apply for active nor support: {len(df_country[(df_country['ACTIVE_MEMBER'] == '') & (df_country['SUPPORT_MEMBER'] == 'No')])}\")\n",
    "\n",
    "# save members from filtered list to a file: Name, Affiliation1, Country1\n",
    "df_filtered = df_filtered[[\"Name\", \"Affiliation1\", \"Country1\", \"EMAIL\"]]\n",
    "if affil:\n",
    "    file_country = f\"members_from_{country.replace(' ', '_')}_{affil.replace(' ', '_')}.xlsx\"\n",
    "else:\n",
    "    file_country = f\"members_from_{country.replace(' ', '_')}.xlsx\"\n",
    "df_filtered.to_excel(file_country, index=False)  \n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie chart for country distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Active Membership by Country — Full Pie + Subtle Per-Wedge Shadow (Fix B)\n",
    "#     + Counts inside + Straight Elbow Leaders + Dot on Wedge\n",
    "membership_toplot = \"all\"\n",
    "if membership_toplot == \"active\":\n",
    "    df_source = df_active\n",
    "    title       = \"Active Membership by Country\"\n",
    "elif membership_toplot == \"support\":\n",
    "    df_source = df_support\n",
    "    title       = \"Support Membership by Country\"      \n",
    "elif membership_toplot == \"all\":\n",
    "    df_source = df_clean\n",
    "    title       = \"All Membership by Country\"\n",
    "\n",
    "# ---- Source & columns ----\n",
    "country_col = \"Country1\"\n",
    "\n",
    "# ---- Labeling rule (choose Top-N or min %)\n",
    "top_n       = 10                 # labels for Top N countries\n",
    "min_percent = None               # or set e.g., 3.0 and set top_n=None\n",
    "group_other = True\n",
    "\n",
    "# ---- Build distribution ----\n",
    "counts = (\n",
    "    df_source[country_col]\n",
    "    .dropna()\n",
    "    .astype(str).str.strip()\n",
    "    .replace({\"\": \"Unknown\"})\n",
    "    .value_counts()\n",
    ")\n",
    "if counts.empty or counts.sum() == 0:\n",
    "    raise ValueError(\"No active-member country data to plot.\")\n",
    "\n",
    "total = counts.sum()\n",
    "perc  = counts / total * 100.0\n",
    "\n",
    "# Decide which slices get OUTSIDE labels (country + %)\n",
    "if min_percent is not None:\n",
    "    label_mask = perc >= float(min_percent)\n",
    "elif top_n is not None:\n",
    "    label_mask = counts.rank(method=\"first\", ascending=False) <= int(top_n)\n",
    "else:\n",
    "    label_mask = counts.rank(method=\"first\", ascending=False) <= 8\n",
    "\n",
    "labeled_counts   = counts[label_mask].sort_values(ascending=False)\n",
    "unlabeled_counts = counts[~label_mask]\n",
    "\n",
    "# Group small ones into \"Other\" (single aggregate)\n",
    "if group_other and not unlabeled_counts.empty:\n",
    "    data_counts = pd.concat([labeled_counts, pd.Series({\"Other\": unlabeled_counts.sum()})])\n",
    "else:\n",
    "    data_counts = counts\n",
    "\n",
    "data_perc = (data_counts / data_counts.sum()) * 100.0\n",
    "\n",
    "# ---- Inside numbers: show member counts inside each wedge (non-bold)\n",
    "def autopct_counts(pct):\n",
    "    return f\"{int(round(pct * data_counts.sum() / 100.0))}\"\n",
    "\n",
    "# Slight explode for the largest slices enhances depth\n",
    "explode = [0.03 if (n in labeled_counts.index) or (n == \"Other\") else 0.0\n",
    "           for n in data_counts.index]\n",
    "\n",
    "# ---- Plot: FULL PIE, no global shadow (Fix B uses per-wedge patch shadows)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "colors = plt.cm.Set3.colors\n",
    "\n",
    "wedges, texts, autotexts = plt.pie(\n",
    "    data_counts.values,\n",
    "    labels=None,                      # we place outside labels manually\n",
    "    autopct=autopct_counts,           # counts inside\n",
    "    pctdistance=0.65,\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    "    counterclock=False,\n",
    "    explode=explode,\n",
    "    shadow=False,                     # << NO big global shadow\n",
    "    textprops=dict(fontsize=10, fontweight=\"normal\", color=\"black\"),\n",
    "    wedgeprops=dict(linewidth=0.6, edgecolor=\"white\")\n",
    ")\n",
    "\n",
    "# Inside counts in normal weight\n",
    "for t in autotexts:\n",
    "    t.set_fontweight(\"light\")\n",
    "\n",
    "# ---- Add a subtle per-wedge shadow \n",
    "#     Small offset + low alpha → depth without wide blooms when saving\n",
    "for w in wedges:\n",
    "    w.set_path_effects([\n",
    "        pe.SimplePatchShadow(offset=(3, -3), shadow_rgbFace=(0, 0, 0, 0.15)),\n",
    "        pe.Normal()\n",
    "    ])\n",
    "\n",
    "# ---- Outside labels: two texts so we can set different weights\n",
    "country_fp = FontProperties(weight=\"normal\")\n",
    "percent_fp = FontProperties(weight=\"light\")  # fallback if no 'light': FontProperties(weight=300)\n",
    "\n",
    "# Which wedges get outside labels?\n",
    "outside_mask = [(n == \"Other\") or (n in labeled_counts.index) for n in data_counts.index]\n",
    "\n",
    "# ---- Connector geometry (straight elbow leaders)\n",
    "r_edge   = 1.02   # contact point on wedge edge\n",
    "r_elbow  = 1.12   # elbow radius (radial segment ends here)\n",
    "r_text   = 1.26   # horizontal label anchor\n",
    "hpad     = 0.06   # little gap so text doesn't sit on the line\n",
    "dot_radius_offset = 0.015  # push dot slightly above the wedge\n",
    "dot_size          = 18\n",
    "dot_edgecolor     = \"white\"\n",
    "dot_linewidth     = 0.8\n",
    "\n",
    "# Connector styling\n",
    "line_color  = \"gray\"\n",
    "line_width  = 1.4\n",
    "joinstyle   = \"miter\"  # crisp corner\n",
    "capstyle    = \"butt\"   # square ends\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "for w, name, pct, show in zip(wedges, data_counts.index, data_perc.values, outside_mask):\n",
    "    if not show:\n",
    "        continue\n",
    "\n",
    "    # mid-angle of wedge\n",
    "    theta = np.deg2rad((w.theta1 + w.theta2) / 2.0)\n",
    "    ux, uy = np.cos(theta), np.sin(theta)\n",
    "\n",
    "    # Points for the polyline connector: wedge → elbow → horizontal\n",
    "    x0, y0 = r_edge * ux,  r_edge * uy         # wedge exit\n",
    "    x1, y1 = r_elbow * ux, r_elbow * uy        # elbow point\n",
    "    x2 = r_text if ux >= 0 else -r_text\n",
    "    y2 = y1                                     # horizontal segment at elbow height\n",
    "    ha = \"left\" if ux >= 0 else \"right\"\n",
    "\n",
    "    # Single polyline path ensures perfectly straight elbow (no curvature)\n",
    "    verts = [(x0, y0), (x1, y1), (x2, y2)]\n",
    "    codes = [Path.MOVETO, Path.LINETO, Path.LINETO]\n",
    "    connector = PathPatch(\n",
    "        Path(verts, codes),\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=line_color,\n",
    "        linewidth=line_width,\n",
    "        joinstyle=joinstyle,\n",
    "        capstyle=capstyle,\n",
    "        zorder=6\n",
    "    )\n",
    "    ax.add_patch(connector)\n",
    "\n",
    "    # Dot on the wedge end (match wedge color for polish)\n",
    "    dot_color = w.get_facecolor()\n",
    "    x_dot = (r_edge + dot_radius_offset) * ux\n",
    "    y_dot = (r_edge + dot_radius_offset) * uy\n",
    "    ax.scatter([x_dot], [y_dot], s=dot_size, color=dot_color,\n",
    "               edgecolors=dot_edgecolor, linewidths=dot_linewidth, zorder=7)\n",
    "\n",
    "    # Two separate texts so we can set different weights:\n",
    "    x_text = x2 + (hpad if ha == \"left\" else -hpad)\n",
    "    # small vertical offsets to stack lines neatly\n",
    "    ax.text(x_text, y2 + 0.04, f\"{name}\",\n",
    "            ha=ha, va=\"center\", fontsize=10, color=\"black\",\n",
    "            fontproperties=country_fp, zorder=7)\n",
    "    ax.text(x_text, y2 - 0.02, f\"{pct:.1f}%\",\n",
    "            ha=ha, va=\"center\", fontsize=10, color=\"black\",\n",
    "            fontproperties=percent_fp, zorder=7)\n",
    "\n",
    "# ---- Final touches & (optional) save\n",
    "plt.title(title, fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save with tight bbox to avoid clipping long leaders/labels\n",
    "# (uncomment to save)\n",
    "# fig.savefig(\"NASC_Active_Membership_by_Country_PIE_elbow_fixB.png\",\n",
    "#             dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save figure\n",
    "if save_figs:   \n",
    "    fig.savefig(\n",
    "        f\"NASC_{membership_toplot}_Membership_by_Country_PIE.png\",\n",
    "        dpi=300, bbox_inches=\"tight\", facecolor=\"white\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot distribution of ACTIVE & CHAIR members by WG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram with the number of ACTIVE_MEMBER per WG\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# get number of active members per WG: split by \";\" and explode\n",
    "df_active[\"ACTIVE_MEMBER\"].str.split(\";\").explode().value_counts().plot(kind='bar', label=f\"Active members ({total_active})\", color=\"purple\", alpha=0.5)\n",
    "# print number of active members per WG\n",
    "print(df_active[\"ACTIVE_MEMBER\"].str.split(\";\").explode().value_counts())\n",
    "\n",
    "# save number of active members per WG to a csv file\n",
    "df_active[\"ACTIVE_MEMBER\"].str.split(\";\").explode().value_counts().to_csv(\"NASC_active_members_by_SWG.csv\", header=[\"Number of Active Members\"])\n",
    "\n",
    "\n",
    "# add in the same plot number of CHAIR_REQUESTED per WG\n",
    "df_chair_req[\"CHAIR_REQUESTED\"].str.split(\";\").explode().value_counts().plot(kind='bar', alpha=0.5, color='green', label=f\"Chairs Requested ({total_chairs_req})\")\n",
    "print(df_chair_req[\"CHAIR_REQUESTED\"].str.split(\";\").explode().value_counts())\n",
    "\n",
    "# add informative box text with WG description in the top right corner\n",
    "labels = df_clean0[\"ACTIVE_MEMBER\"].str.split(\";\").explode().value_counts().index\n",
    "# remove empty strings and leading/trailing spaces\n",
    "labels = [x for x in labels if x]\n",
    "labels = [x.strip() for x in labels]\n",
    "#sort labels and get unique values\n",
    "labels = list(set(labels))\n",
    "labels.sort()\n",
    "plt.text(0.55, 0.7, \"\\n\".join(labels), fontsize=9, transform=plt.gcf().transFigure)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "# set xlabel: \"Working Group\"\n",
    "plt.xlabel(\"Working Group\")\n",
    "# set ylabel: \"Number of active members\"\n",
    "plt.ylabel(\"Number of members\")\n",
    "plt.title(\"Active members by Working Group\")\n",
    "\n",
    "# add legend\n",
    "plt.legend(loc=\"upper right\", bbox_to_anchor=(0.5, 1.0))\n",
    "# add the number of chairs as a text at the top of the red bars\n",
    "for i, v in enumerate(df_chair_req[\"CHAIR_REQUESTED\"].str.split(\";\").explode().value_counts()):\n",
    "    plt.text(i, v + 1, str(v), color='green', ha='center')\n",
    "\n",
    "# Save the plot to a file\n",
    "if save_figs:\n",
    "    plt.savefig(\"NASC_membership_by_WG.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Distribution By Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot of requested chairs per \"Gender\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# support members: purple\n",
    "df_support[\"Gender\"].value_counts().plot(kind='bar', position=0.3,label=f'Support Members ({total_support})', color='blue', alpha=0.5, logy=True)\n",
    "# add label with percentage of support members for each gender\n",
    "for i, v in enumerate(df_support[\"Gender\"].value_counts()):\n",
    "\tpercent = v / total_support\n",
    "\tyloc = v + 5\n",
    "\tif percent < 0.05:\n",
    "\t\tyloc = v + 1\n",
    "\tplt.text(i+0.1, yloc, f\"{v} ({percent:.1%})\", ha='center', color='blue')\n",
    "\n",
    "# active members:blue\n",
    "df_active[\"Gender\"].value_counts().plot(kind='bar', position=0.5, label=f'Active Members ({total_active})', color='purple',alpha=0.5, logy=True)\n",
    "# add label with percentage of active members for each gender\n",
    "total_active = len(df_active)\n",
    "for i, v in enumerate(df_active[\"Gender\"].value_counts()):\n",
    "\tpercent = v / total_active\n",
    "\tyloc = v + 5\n",
    "\tif percent < 0.05:\n",
    "\t \tyloc = v + 1\n",
    "\tplt.text(i, yloc, f\"{v} ({percent:.1%})\", ha='center', color='purple')\n",
    "\n",
    "# chairs:red\n",
    "df_chair_req[\"Gender\"].value_counts().plot(kind='bar',position=0.1, label=f'Requested Chairs ({total_chairs_req})', color=\"green\", alpha=0.5, logy=True)\n",
    "# add label with percentage of active members for each gender\n",
    "for i, v in enumerate(df_chair_req[\"Gender\"].value_counts()):\n",
    "\tpercent = v / total_chairs_req\n",
    "\tcolor = 'lightgreen'\n",
    "\tyloc = v + 5\n",
    "\tif percent < 0.2:\n",
    "\t\tyloc = v + 0.4\n",
    "\tplt.text(i+0.2, yloc, f\"{v} ({percent:.1%})\", ha='center', color=color)\n",
    "\n",
    "\n",
    "# get xticks values\n",
    "xticks = plt.xticks()[0]\n",
    "# get xticks labels (3rd element, once separated by commas)\n",
    "xticklabels = plt.xticks()[1]\n",
    "#separate by commas\n",
    "xticklabels = [x.get_text() for x in xticklabels]\n",
    "# replace empty ticks with \"Unanswered\" and \"Gender diverse **\" by \"Gender diverse\"\n",
    "xticklabels = [\"Unanswered\" if x == \"\" else x for x in xticklabels]\n",
    "xticklabels = [\"Gender diverse\" if x == \"Gender diverse (gender non-conforming and/or transgender)\" else x for x in xticklabels]\n",
    "plt.xticks(xticks, xticklabels)\n",
    "plt.title(\"Distribution by Gender\")\n",
    "# set xlabel\n",
    "plt.xlabel(\"\")\n",
    "# set ylabel\n",
    "plt.ylabel(\"Number of members\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# Save the plot to a file\n",
    "if save_figs:\n",
    "\tplt.savefig(\"NASC_membership_by_Gender.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total members per seniority: first merge \"Mr\", \"Ms\" and \"Mx\" in same category \"Mr/Ms/Mx\"\n",
    "merged_name = \"Pre-Doc\"\n",
    "title_mapping = {\n",
    "    'Mr': merged_name,\n",
    "    'Ms': merged_name,\n",
    "    'Mx': merged_name,\n",
    "    'Dr': 'Dr',\n",
    "    'Prof.': 'Prof.'\n",
    "}\n",
    "\n",
    "# Apply the mapping to a new column\n",
    "df_clean[\"MergedTitle\"] = df_clean[\"Title\"].map(title_mapping)\n",
    "df_support = df_clean[df_clean[\"SUPPORT_MEMBER\"] == \"Yes\"]\n",
    "df_chair_req = df_clean[df_clean[\"CHAIR_REQUESTED\"] != \"\"]\n",
    "df_active = df_clean[df_clean[\"ACTIVE_MEMBER\"] != \"\"]\n",
    "\n",
    "all_titles = df_clean[\"MergedTitle\"].value_counts()\n",
    "\n",
    "# Sort titles by total members (descending)\n",
    "sorted_titles = all_titles.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n",
    "# Build aligned counts\n",
    "total = df_clean[\"MergedTitle\"].value_counts().reindex(sorted_titles, fill_value=0)\n",
    "active = df_active[\"MergedTitle\"].value_counts().reindex(sorted_titles, fill_value=0)\n",
    "support = df_support[\"MergedTitle\"].value_counts().reindex(sorted_titles, fill_value=0)\n",
    "chair_req = df_chair_req[\"MergedTitle\"].value_counts().reindex(sorted_titles, fill_value=0)\n",
    "\n",
    "# Combine into one DataFrame\n",
    "counts_df = pd.DataFrame({\n",
    "    f'Total members ({total_members})': total,\n",
    "    f'Active members ({total_active})': active,\n",
    "    f'Support members ({total_support})': support,\n",
    "    f'Requested Chair ({total_chairs_req})': chair_req\n",
    "})\n",
    "\n",
    "bars_colors = ['red', 'purple', 'blue', 'green']\n",
    "\n",
    "# Plot and add label to each barplot\n",
    "ax = counts_df.plot(kind='bar', width=0.4, logy=True, figsize=(10, 6), color=bars_colors, alpha=0.5)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Number of Members\")\n",
    "ax.set_title(\"NASC Membership by Seniority\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "if save_figs:\n",
    "    plt.savefig(\"NASC_Membership_by_Seniority.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list with the members of each WG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select members of each WG (including chairs)\n",
    "for wg in [\"WG1\", \"WG2\", \"WG3\", \"WG4\", \"WG5\", \"WG6\", \"WG7\"]:\n",
    "    # get members of each WG (being active or chair)\n",
    "    df_wg = df_clean[df_clean[\"ACTIVE_MEMBER\"].str.contains(wg, na=False) | df_clean[\"CHAIR_REQUESTED\"].str.contains(wg, na=False)]\n",
    "    # print members of each WG\n",
    "    #print(f\"Members of {wg}:\")\n",
    "    #print(df_wg[[\"Name\", \"Country1\", \"ACTIVE_MEMBER\", \"EMAIL\"]])\n",
    "    # save to file\n",
    "    #df_wg.to_excel(f\"{wg}_members_{date}.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-match with members of the repository\n",
    "\n",
    "Using data download form UniGe repository, check how many 'old' members are now in the new NASC community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_repo = \"j2xml2112020250625082623.xml\"\n",
    "# Load the XML file into a DataFrame\n",
    "df_repo = pd.read_xml(xml_repo)\n",
    "print(df_repo.columns)\n",
    "\n",
    "#extract columns of interest: name, email\n",
    "df_repo = df_repo[[\"name\", \"email\"]]\n",
    "#print(df_repo.head())\n",
    "# remove rows with empty 'name' or 'email' columns or with None values\n",
    "df_repo = df_repo.dropna(subset=['name', 'email'])\n",
    "df_repo = df_repo[(df_repo['name'] != '') & (df_repo['email'] != '') & (df_repo['name'] != 'None')]\n",
    "\n",
    "# extract surname from name\n",
    "surname_prefixes = ['van', 'von', 'de', 'den', 'del', 'della', 'di', 'la', 'le', 'du', 'da']\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(surname_prefixes) + r')\\b(?:\\s+\\b\\w+\\b)+|\\b\\w+$', re.IGNORECASE)\n",
    "# Function to extract surname\n",
    "def extract_surname(name):\n",
    "    match = pattern.search(name)\n",
    "    return match.group(0) if match else None\n",
    "df_repo['surname'] = df_repo['name'].apply(extract_surname)\n",
    "\n",
    "# remove leading and trailing spaces in 'name' column\n",
    "df_repo['surname'] = df_repo['surname'].str.strip()\n",
    "# change surname to lowercase\n",
    "df_repo['surname'] = df_repo['surname'].str.lower()\n",
    "# change 'email' to lower case\n",
    "df_repo['email_lower'] = df_repo['email'].str.lower()\n",
    "# remove leading and trailing spaces in 'email' column\n",
    "df_repo['email_lower'] = df_repo['email_lower'].str.strip()\n",
    "print(df_repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-match with members of the df_clean DataFrame\n",
    "# select only interesting columns from df_clean: 'Name', 'EMAIL'\n",
    "df_active_for_repo = df_active[['Name', 'Last Name', 'EMAIL', 'ACTIVE_MEMBER']].copy()\n",
    "# change 'Last Name' to lowercase\n",
    "df_active_for_repo['LastName_lower'] = df_active_for_repo['Last Name'].str.lower()\n",
    "# remove leading and trailing spaces in 'Last Name' column\n",
    "df_active_for_repo['LastName_lower'] = df_active_for_repo['LastName_lower'].str.strip()\n",
    "# change 'EMAIL' to lower case\n",
    "df_active_for_repo['EMAIL_lower'] = df_active_for_repo['EMAIL'].str.lower()\n",
    "# remove leading and trailing spaces in 'EMAIL' column\n",
    "df_active_for_repo['EMAIL_lower'] = df_active_for_repo['EMAIL_lower'].str.strip()\n",
    "\n",
    "print(df_active_for_repo.columns)\n",
    "print(df_repo.columns)\n",
    "\n",
    "# cross-match members with email coincidence \n",
    "cross_match = df_active_for_repo.merge(df_repo, left_on=['EMAIL_lower'], right_on=['email_lower'], how='inner')\n",
    "# open file to save cross-matched members\n",
    "cross_match_tofile = cross_match.copy()\n",
    "cross_match_tofile = cross_match_tofile.drop(columns=['Last Name','LastName_lower', 'surname', 'email_lower', 'EMAIL_lower'])\n",
    "cross_match_tofile.rename(columns={'Name': 'Name_NASC', 'EMAIL': 'EMAIL_NASC', 'name':'Name_REPO','email': 'email_REPO'}, inplace=True)\n",
    "#cross_match_tofile.to_excel(f\"cross_matched_members_NASC_repo.xlsx\", index=False)\n",
    "print(f\"Cross-matched members (by email): {len(cross_match)}\")\n",
    "print(\"=====================================================\")\n",
    "# for each cross-matched member, print entry in df_clean and df_repo\n",
    "for index, row in cross_match.iterrows():\n",
    "    #print(f\"Cross-matched member: {row['Name']}\")\n",
    "    entry_NASC = df_active_for_repo[df_active_for_repo['EMAIL_lower'] == row['EMAIL_lower']].to_dict(orient='records')\n",
    "    entry_repo = df_repo[df_repo['email_lower'] == row['EMAIL_lower']].to_dict(orient='records')\n",
    "    #print(f\"    Entry in NASC: {entry_NASC}\")\n",
    "    #print(f\"    Entry in REPO: {entry_repo}\")\n",
    "# remove entry_NASC and entry_repo from df_active_for_repo and df_repo\n",
    "df_active_for_repo_remaining = df_active_for_repo[~df_active_for_repo.set_index(['EMAIL_lower']).index.isin(cross_match.set_index(['EMAIL_lower']).index)]\n",
    "df_repo_remaining = df_repo[~df_repo.set_index(['email_lower']).index.isin(cross_match.set_index(['email_lower']).index)] \n",
    "\n",
    "\n",
    "# get cross-matched members with name coincidence but not email coincidence\n",
    "cross_match_name_only = df_active_for_repo_remaining.merge(df_repo_remaining, left_on='LastName_lower', right_on='surname', how='inner')\n",
    "cross_match_name_only = cross_match_name_only[cross_match_name_only['EMAIL_lower'] != cross_match_name_only['email_lower']]\n",
    "print(f\"Cross-matched members (Name only): {len(cross_match_name_only)}\")\n",
    "print(\"=====================================================\")\n",
    "#print(cross_match_name_only)\n",
    "# for each cross-matched member, print entry in df_clean and df_repo\n",
    "for index, row in cross_match_name_only.iterrows():\n",
    "    #print(f\"Cross-matched member (Name only): {row['Name']}\")\n",
    "    entry_NASC = df_active_for_repo_remaining[df_active_for_repo_remaining['LastName_lower'] == row['LastName_lower']].to_dict(orient='records')\n",
    "    entry_repo = df_repo_remaining[df_repo_remaining['surname'] == row['LastName_lower']].to_dict(orient='records')\n",
    "    #print(f\"    Entry in NASC: {entry_NASC}\")\n",
    "    #print(f\"    Entry in REPO: {entry_repo}\")\n",
    "cross_match_name_only_tofile = cross_match_name_only.copy()\n",
    "# drop columns LasName_lower and surname\n",
    "cross_match_name_only_tofile = cross_match_name_only_tofile.drop(columns=['Last Name','LastName_lower', 'surname', 'email_lower', 'EMAIL_lower'])\n",
    "# rename 'Name' to 'Name_NASC', 'EMAIL' to EMAIL_NASC and 'email' to 'email_REPO'\n",
    "cross_match_name_only_tofile.rename(columns={'Name': 'Name_NASC', 'EMAIL': 'EMAIL_NASC', 'name':'Name_REPO','email': 'email_REPO'}, inplace=True)\n",
    "cross_match_name_only_tofile.to_excel(f\"cross_matched_members_NASC_repo_change_email.xlsx\", index=False)\n",
    "df_active_for_repo_remaining_remaining = df_active_for_repo_remaining[~df_active_for_repo_remaining.set_index(['LastName_lower']).index.isin(cross_match_name_only.set_index(['LastName_lower']).index)]\n",
    "df_repo_remaining_remaining = df_repo_remaining[~df_repo_remaining.set_index(['surname']).index.isin(cross_match_name_only.set_index(['surname']).index)] \n",
    "\n",
    "# save remaining members to a file\n",
    "df_active_for_repo_remaining_remaining_tofile = df_active_for_repo_remaining_remaining.copy()\n",
    "df_active_for_repo_remaining_remaining_tofile = df_active_for_repo_remaining_remaining_tofile.drop(columns=['Last Name','LastName_lower', 'EMAIL_lower'])\n",
    "df_active_for_repo_remaining_remaining_tofile.rename(columns={'Name': 'Name_NASC', 'EMAIL': 'EMAIL_NASC', 'name':'Name_REPO','email': 'email_REPO'}, inplace=True)\n",
    "df_repo_remaining_remaining_tofile = df_repo_remaining_remaining.copy()\n",
    "df_repo_remaining_remaining_tofile = df_repo_remaining_remaining_tofile.drop(columns=['surname', 'email_lower'])\n",
    "df_repo_remaining_remaining_tofile.rename(columns={'name': 'Name_REPO', 'email': 'email_REPO'}, inplace=True)\n",
    "\n",
    "df_active_for_repo_remaining_remaining_tofile.to_excel(f\"remaining_members_NASC_notREPO.xlsx\", index=False)\n",
    "df_repo_remaining_remaining_tofile.to_excel(f\"remaining_members_REPO_notNASC.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check particular cases\n",
    "surname = \"den Herder\"\n",
    "surname_lower = surname.lower()\n",
    "\n",
    "# look for it in df_clean\n",
    "df_check = df_active_for_repo_remaining_remaining[df_active_for_repo_remaining_remaining['LastName_lower'] == surname_lower]\n",
    "print(f\"Members with required LastName_lower '{surname}': {len(df_check)}\")\n",
    "print(df_check[['Name', 'LastName_lower', 'EMAIL']])\n",
    "# look for it in df_repo\n",
    "df_check_repo = df_repo_remaining_remaining[df_repo_remaining_remaining['surname'] == surname_lower]\n",
    "print(f\"Members with required surname '{surname}': {len(df_check_repo)}\")\n",
    "print(df_check_repo[['name', 'surname', 'email']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices   \n",
    "Extra work (To be reviewed with new registrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# keep columns: ID, WG0, WG1, WG2\n",
    "df = df[[\"ID\", \"WG0\", \"WG1\", \"WG2\"]]\n",
    "# Rename column WG0 to \"Primary WG\"\n",
    "df.rename(columns={\"WG0\": \"Primary WG\"}, inplace=True)\n",
    "# Rename column WG1 to \"Secondary WG\"\n",
    "df.rename(columns={\"WG1\": \"Secondary WG\"}, inplace=True)\n",
    "# Rename column WG2 to \"Tertiary WG\"\n",
    "df.rename(columns={\"WG2\": \"Tertiary WG\"}, inplace=True)\n",
    "# Replace cells content in these columns:\n",
    "# \"WG1: Large-scale structure of the Universe\" -> \"WG1\"\n",
    "# \"WG2: Galaxies and supermassive black holes\" -> \"WG2\"\n",
    "# \"WG3: Stars and their environment\" -> \"WG3\"\n",
    "# \"WG4: Compact objects\" -> \"WG4\"\n",
    "# \"WG5: Transients and multi-messenger astrophysics\" -> \"WG5\"\n",
    "# \"WG6: Cosmology and fundamental physics\" -> \"WG6\"\n",
    "# \"WG7: Science Support\" -> \"WG7\"\n",
    "\n",
    "# Strip leading and trailing whitespaces or tabs\n",
    "df[\"Primary WG\"] = df[\"Primary WG\"].str.strip()\n",
    "df[\"Secondary WG\"] = df[\"Secondary WG\"].str.strip()\n",
    "df[\"Tertiary WG\"] = df[\"Tertiary WG\"].str.strip()\n",
    "# in each column replace the content of the cell with the first 3 letters of the cell content\n",
    "df[\"Primary WG\"] = df[\"Primary WG\"].str[:3]\n",
    "df[\"Secondary WG\"] = df[\"Secondary WG\"].str[:3]\n",
    "df[\"Tertiary WG\"] = df[\"Tertiary WG\"].str[:3]\n",
    "\n",
    "# remove rows with empty cells in all the groups\n",
    "df = df.dropna(subset=[\"Primary WG\", \"Secondary WG\", \"Tertiary WG\"], how=\"all\")\n",
    "\n",
    "# keep only the rows with an empty cell in the \"Tertiary WG\" column\n",
    "df = df[df[\"Tertiary WG\"].isnull()]\n",
    "# remove the \"Tertiary WG\" column\n",
    "df = df.drop(columns=[\"Tertiary WG\"])\n",
    "# remove rows with empty cells in all the groups\n",
    "df = df.dropna(subset=[\"Primary WG\", \"Secondary WG\"], how=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salva el dataframe en un archivo CSV\n",
    "df.to_csv(\"membership_clean2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los nombres de las columnas\n",
    "df = pd.read_csv(\"membership_clean2.csv\", header=0)\n",
    "col1 = \"ID\"\n",
    "col2 = \"Primary WG\"\n",
    "col3 = \"Secondary WG\"\n",
    "\n",
    "# Reemplazar valores NaN con strings vacíos y asegurarse de que son strings\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# Obtener la lista única de WGs excluyendo valores vacíos\n",
    "wgs = sorted(set(df[col2]).union(df[col3]) - {\"\"})\n",
    "\n",
    "# Crear una matriz de ceros de tamaño (n_WGs x n_WGs)\n",
    "matrix = pd.DataFrame(np.zeros((len(wgs), len(wgs))), index=wgs, columns=wgs)\n",
    "\n",
    "# Llenar la matriz contando las asociaciones\n",
    "for _, row in df.iterrows():\n",
    "    selected_wgs = {row[col2], row[col3]} - {\"\"}  # Filtrar valores vacíos\n",
    "    \n",
    "     # Llenar la diagonal con personas que solo pertenecen a un WG\n",
    "    if len(selected_wgs) == 1:\n",
    "        wg = list(selected_wgs)[0]\n",
    "        matrix.loc[wg, wg] += 1  # Contar personas que solo eligieron este WG\n",
    "    \n",
    "    # no contar dos veces la misma asociación\n",
    "    elif len(selected_wgs) == 2:\n",
    "        wg1, wg2 = selected_wgs\n",
    "        matrix.loc[wg1, wg2] += 1\n",
    "        matrix.loc[wg2, wg1] += 1\n",
    "    \n",
    "\n",
    "# Graficar el heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(matrix, annot=True, cmap=\"Blues\", linewidths=0.5, fmt=\".0f\", cbar=True)\n",
    "\n",
    "# Configuración del gráfico\n",
    "plt.title(\"WGs association heatmap\")\n",
    "plt.xlabel(\"WG\")\n",
    "plt.ylabel(\"WG\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Mostrar\n",
    "plt.show()\n",
    "# Guardar la figura\n",
    "plt.savefig(\"heatmap2.png\")\n",
    "#cerrar la figura\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un grafo vacio\n",
    "G = nx.Graph()\n",
    "# Agregar los nodos al grafo: \n",
    "# usa solo las columnas \"Primary WG\", \"Secondary WG\". No uses \"Tertiary WG\"\n",
    "G.add_nodes_from(df[[\"Primary WG\", \"Secondary WG\"]].dropna().values.flatten())\n",
    "\n",
    "# Agregar las aristas al grafo: solo si los valores no son None\n",
    "# usa solo las columnas \"Primary WG\", \"Secondary WG\". No uses \"Tertiary WG\"\n",
    "G.add_edges_from(df[[\"Primary WG\", \"Secondary WG\"]].dropna().values)\n",
    "# Dibujar el grafo: haz los nodos mas grandes y usa el layout \"spring\" y evita mucho espacio en blanco\n",
    "plt.figure(figsize=(10, 10))\n",
    "nx.draw(G, node_size=5000, pos=nx.spring_layout(G), with_labels=True)\n",
    "# add text box with the WGs names\n",
    "# \"WG1: Large-scale structure of the Universe\"\n",
    "# \"WG2: Galaxies and supermassive black holes\"\n",
    "# \"WG3: Stars and their environment\"\n",
    "# \"WG4: Compact objects\"\n",
    "# \"WG5: Transients and multi-messenger astrophysics\"\n",
    "# \"WG6: Cosmology and fundamental physics\"\n",
    "# \"WG7: Science Support\"\n",
    "fsize = 10\n",
    "plt.text(0.5, 0.5, \"WG1: Large-scale structure of the Universe\", fontsize=fsize, ha='center')\n",
    "plt.text(0.5, 0.45, \"WG2: Galaxies and supermassive black holes\", fontsize=fsize, ha='center')\n",
    "plt.text(0.5, 0.4, \"WG3: Stars and their environment\", fontsize=fsize, ha='center')\n",
    "plt.text(0.5, 0.35, \"WG4: Compact objects\", fontsize=fsize, ha='center')\n",
    "plt.text(0.5, 0.3, \"WG5: Transients and multi-messenger astrophysics\", fontsize=fsize, ha='center')\n",
    "plt.text(0.5, 0.25, \"WG6: Cosmology and fundamental physics\", fontsize=fsize, ha='center')\n",
    "plt.text(0.5, 0.2, \"WG7: Science Support\", fontsize=fsize, ha='center')\n",
    "# Guardar la figura en un archivo\n",
    "plt.savefig(\"graph.png\")\n",
    "# Mostrar la figura\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# localiza las líneas donde aparece WG7 en alguna de las columnas\n",
    "df.loc[(df[\"Primary WG\"] == \"WG7\") | (df[\"Secondary WG\"] == \"WG7\") | (df[\"Tertiary WG\"] == \"WG7\")]\n",
    "# cuántos son?\n",
    "len(df.loc[(df[\"Primary WG\"] == \"WG7\") | (df[\"Secondary WG\"] == \"WG7\") | (df[\"Tertiary WG\"] == \"WG7\")])\n",
    "\n",
    "# localiza las líneas donde aparece WG7 en más de una columna\n",
    "df.loc[(df[\"Primary WG\"] == \"WG7\") & (df[\"Secondary WG\"] == \"WG7\") | (df[\"Primary WG\"] == \"WG7\") & (df[\"Tertiary WG\"] == \"WG7\") | (df[\"Secondary WG\"] == \"WG7\") & (df[\"Tertiary WG\"] == \"WG7\")]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GSFCenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
